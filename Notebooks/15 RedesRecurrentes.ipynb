{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2540d9",
   "metadata": {},
   "source": [
    "# Redes recurrentes\n",
    "\n",
    "Las redes recurrentes son una arquitectura que nos permite lidiar con cadenas estocásticas y poder emitir cadenas como salidas. Existen diferentes tipos de redes recurrentes. Aquí implementamos una con la ayuda de torch que se enfoca en procesar datos textuales y etiquetar un texto de acuerdo a la función de las palabras en este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54461756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Funcion que crea un vocabulario de palabras con un indice numerico\n",
    "def vocab():\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    return vocab    \n",
    "\n",
    "#Funcion que pasa la cadena de simbolos a una secuencia con indices numericos\n",
    "def text2numba(corpus, vocab):\n",
    "    for doc in corpus:\n",
    "        yield [vocab[w] for w in doc.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59fd9f",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "Para trabajar con datos textuales, estos requieren de un tratamiento especial. En primer lugar se sustituirán las cadenas de texto por índices numéricos, esto permitirá generar embeddings a partir de una capa de embedding.\n",
    "\n",
    "La indexación se hará tanto en el texto de entrada como el de salida, por tanto tendremos dos vocabularios. Los datos, como se observa, se traducen de una cadena de índices a otra cadena de índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2e7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [3, 5, 6], [0, 5, 7, 8, 9], [0, 1, 2, 10], [3, 1, 2, 11], [0, 1, 2], [0, 12, 2, 11], [3, 12, 2], [13, 14, 10], [0, 14], [3, 14], [13, 14, 3, 14], [0, 12, 2, 10]]\n",
      "[[0, 1, 2, 3, 1], [3, 1, 2], [0, 1, 2, 0, 1], [3, 1, 2, 1], [0, 1, 2, 4], [0, 1, 2], [0, 1, 2, 1], [3, 1, 2], [5, 2, 4], [0, 1], [3, 1], [5, 2, 3, 1], [0, 1, 2, 4]]\n"
     ]
    }
   ],
   "source": [
    "#Entradas\n",
    "inputs = ['el perro come un hueso', 'un muchacho jugaba', 'el muchacho saltaba la cuerda', 'el perro come mucho',\n",
    "          'un perro come croquetas', 'el perro come', 'el gato come croquetas', 'un gato come', 'yo juego mucho', \n",
    "          'el juego', 'un juego', 'yo juego un juego', 'el gato come mucho']\n",
    "\n",
    "#Etiquetas de salida\n",
    "outputs = ['DA NC V DD NC', 'DD NC V', 'DA NC V DA NC', 'DD NC V NC', 'DA NC V Adv', 'DA NC V', 'DA NC V NC', \n",
    "           'DD NC V', 'DP V Adv', 'DA NC', 'DD NC', 'DP V DD NC', 'DA NC V Adv']\n",
    "\n",
    "#Llamamos la funcion para crear el vocabulario\n",
    "in_voc = vocab()\n",
    "#Creamos el vocabulario y le asignamos un indice a cada simbolo segun su aparicion\n",
    "x = list(text2numba(inputs,in_voc))\n",
    "\n",
    "#Vocabulario de emisiones\n",
    "out_voc = vocab()\n",
    "#Se susituyen las emisiones por sus índices numéricos\n",
    "y = list(text2numba(outputs,out_voc))\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d55af",
   "metadata": {},
   "source": [
    "## Red recurrente\n",
    "\n",
    "Definimos la red recurrente de la siguiente forma:\n",
    "\n",
    "* Capa de embedding: Esta priemra capa generará vectores de embeddings a partir de los índices numéricos de entrada.\n",
    "* Capa recurrente: Es la capa donde se obtienen las recurrencias; esta capa puede ser bidireccional o sólo en la dirección de avance. Ocuparemos además una función de activación para la capa recurrente.\n",
    "* Capa de salida: La capa con activación Sofmax para obtener los valores de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3196fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim=100, dim_h=200):\n",
    "        super().__init__()\n",
    "        #Capa de embedding\n",
    "        self.emb = nn.Embedding(dim_in,dim)\n",
    "        #Capa de RNN (se toma bidireccional)\n",
    "        self.recurrence = nn.RNN(dim, dim_h, bidirectional=True)\n",
    "        #Salida\n",
    "        self.ffw = nn.Sequential(nn.Linear(2*dim_h,dim_out), nn.Softmax(dim=2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Se pasa a formato torch\n",
    "        x = torch.tensor(x)\n",
    "        #Embedding\n",
    "        x = self.emb(x)\n",
    "        #Ajustes de tamaño\n",
    "        x = x.unsqueeze(1)\n",
    "        #Estados de recurrencia\n",
    "        h, c = self.recurrence(x)\n",
    "        #Activación\n",
    "        h = h.tanh()\n",
    "        #Salida\n",
    "        y_pred = self.ffw(h)\n",
    "        #Se acomoda la salida para que la tome el loss\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20370a",
   "metadata": {},
   "source": [
    "Definiremos la rec recurrente con los parámetros que indiquemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e65dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RecurrentNetwork(len(in_voc), len(out_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f78548",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red\n",
    "\n",
    "Para entrenar la red se utiliza el BPT, <b>Back Propagation Through Time</b>, que es una variación del método de backpropagation. En este caso, el error se propaga a través del tiempo; como está retropropagación está determinada por la arquitectura recurrente de la red, no hace falta indicar en pytorch que se utiliza esta retropropagación, pues el método entiende que se debe considerar los diferentes estados recurrentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533bb0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Numero de iteraciones\n",
    "epochs = 100\n",
    "#La función de riesgo es la entropía cruzada\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#Los parametros que se van a actualizar\n",
    "optimizer = torch.optim.Adagrad(rnn.parameters(), lr=0.1)\n",
    "\n",
    "#Se entrena el modelo\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        #FORWARD\n",
    "        y_pred = rnn(x_i)\n",
    "\n",
    "        #BACKWARD\n",
    "        #Resize de las variables esperadas (se agrega dimension de length_seq)\n",
    "        y_i = (torch.tensor(y_i)).unsqueeze(1)\n",
    "        #Se calcula el eror\n",
    "        loss = criterion(y_pred, y_i)\n",
    "        #zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #Backprop\n",
    "        loss.backward()\n",
    "        #Se actualizan los parametros\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d57420",
   "metadata": {},
   "source": [
    "###  Resultados\n",
    "\n",
    "Ya que la red toma como entrada índices numéricos y emite como salidas también índices numéricos, definiremos funciones que nos permitan aplicar la red a texto para hacer más amigable la interacción con la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881813f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocanulario de indice a texto\n",
    "tagger_voc = {i:w for w,i in out_voc.items()}\n",
    "def tagger(text):\n",
    "    \"\"\"Función que toma el texto aplica la red y arroja las etiquetas textuales.\"\"\"\n",
    "    x_in = [in_voc[w] for w in text.split()]\n",
    "    probs = rnn(x_in)\n",
    "    values = probs.argmax(axis=1)\n",
    "    tags = [tagger_voc[int(v[0])] for v in values]\n",
    "    \n",
    "    return ' '.join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7224197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DP V Adv DD NC'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger('yo juego mucho un juego')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03563f19",
   "metadata": {},
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
